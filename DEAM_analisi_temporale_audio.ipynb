{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Analisi Temporale delle Caratteristiche Audio con Librosa üéµ\n",
    "\n",
    "Questo notebook si concentra sull'estrazione e visualizzazione delle caratteristiche audio nel tempo, mostrando come le propriet√† sonore evolvono durante l'esecuzione di un brano musicale. Analizzeremo come le caratteristiche audio si correlano con le emozioni (arousal e valence) in modo dinamico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparazione dell'ambiente\n",
    "\n",
    "Importiamo le librerie necessarie per l'analisi audio e la visualizzazione dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa  # Libreria principale per l'analisi audio\n",
    "import librosa.display  # Per visualizzare i grafici audio\n",
    "import matplotlib.pyplot as plt  # Per creare grafici\n",
    "import numpy as np  # Per calcoli matematici\n",
    "import pandas as pd  # Per gestire i dati in formato tabellare\n",
    "import seaborn as sns  # Per grafici statistici pi√π avanzati\n",
    "from scipy import stats  # Per calcoli statistici\n",
    "\n",
    "# Impostiamo matplotlib per mostrare grafici pi√π belli\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')  # Utilizziamo uno stile compatibile con le versioni recenti\n",
    "\n",
    "# Impostiamo alcune opzioni di visualizzazione\n",
    "plt.rcParams['figure.figsize'] = (14, 6)  # Dimensione predefinita dei grafici\n",
    "plt.rcParams['font.size'] = 12  # Dimensione del testo nei grafici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento di un file audio e delle annotazioni emozionali\n",
    "\n",
    "Carichiamo un file audio dal dataset DEAM (Database for Emotional Analysis in Music) e le relative annotazioni dinamiche di arousal e valence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso del file audio (da modificare in base alla posizione effettiva)\n",
    "file_path = 'DEAM_audio/MEMD_audio/5.mp3'  # Utilizziamo un file esistente nella cartella MEMD_audio\n",
    "\n",
    "# Carichiamo il file audio\n",
    "# y contiene i dati audio (come un'onda sonora)\n",
    "# sr √® la frequenza di campionamento (quanti punti al secondo)\n",
    "y, sr = librosa.load(file_path, duration=60)  # Limitiamo a 60 secondi per semplicit√†\n",
    "\n",
    "print(f\"Durata audio: {len(y)/sr:.2f} secondi\")\n",
    "print(f\"Frequenza di campionamento: {sr} Hz\")\n",
    "\n",
    "# Simuliamo le annotazioni dinamiche di arousal e valence\n",
    "# In un caso reale, queste verrebbero caricate da file\n",
    "# Nota: nel dataset DEAM, le annotazioni sono tipicamente campionate a 2Hz (ogni 500ms)\n",
    "\n",
    "# Creiamo un array di tempo per le annotazioni (un punto ogni 500ms)\n",
    "annotation_time = np.arange(0, len(y)/sr, 0.5)\n",
    "\n",
    "# Simuliamo valori di arousal (eccitazione) che aumentano e diminuiscono nel tempo\n",
    "# In un caso reale, questi dati verrebbero caricati dal dataset DEAM\n",
    "arousal = 0.5 + 0.4 * np.sin(2 * np.pi * annotation_time / 30)\n",
    "\n",
    "# Simuliamo valori di valence (positivit√†) che variano nel tempo\n",
    "valence = 0.5 + 0.3 * np.sin(2 * np.pi * annotation_time / 20)\n",
    "\n",
    "# Creiamo un DataFrame per le annotazioni\n",
    "annotations = pd.DataFrame({\n",
    "    'time': annotation_time,\n",
    "    'arousal': arousal,\n",
    "    'valence': valence\n",
    "})\n",
    "\n",
    "print(f\"Numero di annotazioni: {len(annotations)}\")\n",
    "annotations.head()  # Mostriamo le prime righe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizzazione della forma d'onda e delle annotazioni emozionali\n",
    "\n",
    "Visualizziamo la forma d'onda del brano insieme alle annotazioni di arousal e valence per avere una prima panoramica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un grafico con 3 sottografici\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Grafico della forma d'onda\n",
    "librosa.display.waveshow(y, sr=sr, ax=ax1)\n",
    "ax1.set_title('Forma d\\'onda del brano')\n",
    "ax1.set_ylabel('Ampiezza')\n",
    "\n",
    "# Grafico dell'arousal nel tempo\n",
    "ax2.plot(annotations['time'], annotations['arousal'], color='red')\n",
    "ax2.set_title('Arousal (eccitazione) nel tempo')\n",
    "ax2.set_ylabel('Arousal (0-1)')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True)\n",
    "\n",
    "# Grafico della valence nel tempo\n",
    "ax3.plot(annotations['time'], annotations['valence'], color='green')\n",
    "ax3.set_title('Valence (positivit√†) nel tempo')\n",
    "ax3.set_xlabel('Tempo (secondi)')\n",
    "ax3.set_ylabel('Valence (0-1)')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estrazione delle caratteristiche audio nel tempo\n",
    "\n",
    "Ora estraiamo diverse caratteristiche audio e le analizziamo nel tempo, utilizzando finestre temporali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la dimensione della finestra per l'analisi temporale (in campioni)\n",
    "# Utilizziamo una finestra di 0.5 secondi per allinearci con le annotazioni\n",
    "frame_length = int(sr * 0.5)  # 0.5 secondi di audio\n",
    "hop_length = frame_length  # Non sovrapponiamo le finestre per semplicit√†\n",
    "\n",
    "# Calcoliamo il numero di frame che otterremo\n",
    "n_frames = 1 + int((len(y) - frame_length) / hop_length)\n",
    "print(f\"Numero di frame temporali: {n_frames}\")\n",
    "\n",
    "# Creiamo array vuoti per memorizzare le caratteristiche estratte\n",
    "frame_times = []  # Tempo centrale di ogni frame\n",
    "rms_values = []  # Root Mean Square (energia)\n",
    "spectral_centroid_values = []  # Centroide spettrale (brillantezza)\n",
    "spectral_contrast_values = []  # Contrasto spettrale (differenza tra picchi e valli)\n",
    "spectral_rolloff_values = []  # Rolloff spettrale (distribuzione dell'energia)\n",
    "\n",
    "# Estraiamo le caratteristiche frame per frame\n",
    "for i in range(n_frames):\n",
    "    # Calcoliamo l'indice di inizio e fine del frame corrente\n",
    "    start = i * hop_length\n",
    "    end = min(start + frame_length, len(y))\n",
    "    \n",
    "    # Estraiamo il segmento audio per questo frame\n",
    "    frame = y[start:end]\n",
    "    \n",
    "    # Calcoliamo il tempo centrale di questo frame (in secondi)\n",
    "    frame_time = (start + (end - start) / 2) / sr\n",
    "    frame_times.append(frame_time)\n",
    "    \n",
    "    # Energia (RMS) - correlata all'intensit√†/volume\n",
    "    rms = np.sqrt(np.mean(frame**2))\n",
    "    rms_values.append(rms)\n",
    "    \n",
    "    # Se il frame √® troppo corto, alcune funzioni potrebbero fallire\n",
    "    # Aggiungiamo controlli per evitare errori\n",
    "    if len(frame) > 0:\n",
    "        # Centroide spettrale - correlato alla brillantezza/acutezza\n",
    "        centroid = np.mean(librosa.feature.spectral_centroid(y=frame, sr=sr))\n",
    "        spectral_centroid_values.append(centroid)\n",
    "        \n",
    "        # Contrasto spettrale - correlato alla differenza tra picchi e valli\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=frame, sr=sr))\n",
    "        spectral_contrast_values.append(contrast)\n",
    "        \n",
    "        # Rolloff spettrale - correlato alla distribuzione dell'energia\n",
    "        rolloff = np.mean(librosa.feature.spectral_rolloff(y=frame, sr=sr))\n",
    "        spectral_rolloff_values.append(rolloff)\n",
    "    else:\n",
    "        # Se il frame √® vuoto, aggiungiamo valori nulli\n",
    "        spectral_centroid_values.append(0)\n",
    "        spectral_contrast_values.append(0)\n",
    "        spectral_rolloff_values.append(0)\n",
    "\n",
    "# Creiamo un DataFrame con tutte le caratteristiche estratte\n",
    "features_df = pd.DataFrame({\n",
    "    'time': frame_times,\n",
    "    'rms': rms_values,\n",
    "    'spectral_centroid': spectral_centroid_values,\n",
    "    'spectral_contrast': spectral_contrast_values,\n",
    "    'spectral_rolloff': spectral_rolloff_values\n",
    "})\n",
    "\n",
    "# Normalizziamo le caratteristiche per facilitare il confronto\n",
    "features_df['rms_norm'] = features_df['rms'] / features_df['rms'].max()\n",
    "features_df['centroid_norm'] = features_df['spectral_centroid'] / features_df['spectral_centroid'].max()\n",
    "features_df['contrast_norm'] = (features_df['spectral_contrast'] - features_df['spectral_contrast'].min()) / (features_df['spectral_contrast'].max() - features_df['spectral_contrast'].min())\n",
    "features_df['rolloff_norm'] = features_df['spectral_rolloff'] / features_df['spectral_rolloff'].max()\n",
    "\n",
    "# Mostriamo le prime righe del DataFrame\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpolazione delle annotazioni emozionali\n",
    "\n",
    "Per confrontare le caratteristiche audio con le annotazioni emozionali, dobbiamo assicurarci che abbiano la stessa frequenza di campionamento. Interpoleremmo le annotazioni per allinearle con i frame audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpoliamo le annotazioni per allinearle con i frame audio\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Funzione di interpolazione per arousal\n",
    "arousal_interp = interp1d(annotations['time'], annotations['arousal'], kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "# Funzione di interpolazione per valence\n",
    "valence_interp = interp1d(annotations['time'], annotations['valence'], kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "# Applichiamo l'interpolazione ai tempi dei frame\n",
    "features_df['arousal'] = arousal_interp(features_df['time'])\n",
    "features_df['valence'] = valence_interp(features_df['time'])\n",
    "\n",
    "# Applichiamo un filtro di smoothing per ridurre il rumore nelle caratteristiche\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Smoothing delle caratteristiche (sigma controlla il livello di smoothing)\n",
    "features_df['rms_smooth'] = gaussian_filter1d(features_df['rms_norm'], sigma=2)\n",
    "features_df['centroid_smooth'] = gaussian_filter1d(features_df['centroid_norm'], sigma=2)\n",
    "features_df['contrast_smooth'] = gaussian_filter1d(features_df['contrast_norm'], sigma=2)\n",
    "features_df['rolloff_smooth'] = gaussian_filter1d(features_df['rolloff_norm'], sigma=2)\n",
    "\n",
    "# Smoothing delle annotazioni\n",
    "features_df['arousal_smooth'] = gaussian_filter1d(features_df['arousal'], sigma=2)\n",
    "features_df['valence_smooth'] = gaussian_filter1d(features_df['valence'], sigma=2)\n",
    "\n",
    "# Mostriamo le prime righe del DataFrame con le nuove colonne\n",
    "features_df[['time', 'rms_smooth', 'centroid_smooth', 'arousal_smooth', 'valence_smooth']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizzazione delle caratteristiche audio nel tempo\n",
    "\n",
    "Visualizziamo come le caratteristiche audio variano nel tempo e come si correlano con le annotazioni emozionali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un grafico con 4 sottografici\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 16), sharex=True)\n",
    "\n",
    "# 1. RMS (energia/volume) e Arousal\n",
    "ax1.plot(features_df['time'], features_df['rms_smooth'], color='blue', label='RMS (energia)')\n",
    "ax1.set_title('Energia (RMS) e Arousal nel tempo')\n",
    "ax1.set_ylabel('RMS normalizzato', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Aggiungiamo Arousal sullo stesso grafico (asse y secondario)\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(features_df['time'], features_df['arousal_smooth'], color='red', label='Arousal')\n",
    "ax1_twin.set_ylabel('Arousal', color='red')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "ax1_twin.set_ylim(0, 1)\n",
    "\n",
    "# Aggiungiamo una legenda\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax1_twin.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# 2. Centroide spettrale (brillantezza) e Valence\n",
    "ax2.plot(features_df['time'], features_df['centroid_smooth'], color='purple', label='Centroide spettrale')\n",
    "ax2.set_title('Brillantezza (Centroide spettrale) e Valence nel tempo')\n",
    "ax2.set_ylabel('Centroide normalizzato', color='purple')\n",
    "ax2.tick_params(axis='y', labelcolor='purple')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Aggiungiamo Valence sullo stesso grafico (asse y secondario)\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(features_df['time'], features_df['valence_smooth'], color='green', label='Valence')\n",
    "ax2_twin.set_ylabel('Valence', color='green')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='green')\n",
    "ax2_twin.set_ylim(0, 1)\n",
    "\n",
    "# Aggiungiamo una legenda\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# 3. Contrasto spettrale e Arousal\n",
    "ax3.plot(features_df['time'], features_df['contrast_smooth'], color='orange', label='Contrasto spettrale')\n",
    "ax3.set_title('Contrasto spettrale e Arousal nel tempo')\n",
    "ax3.set_ylabel('Contrasto normalizzato', color='orange')\n",
    "ax3.tick_params(axis='y', labelcolor='orange')\n",
    "ax3.grid(True)\n",
    "\n",
    "# Aggiungiamo Arousal sullo stesso grafico (asse y secondario)\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(features_df['time'], features_df['arousal_smooth'], color='red', label='Arousal')\n",
    "ax3_twin.set_ylabel('Arousal', color='red')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='red')\n",
    "ax3_twin.set_ylim(0, 1)\n",
    "\n",
    "# Aggiungiamo una legenda\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# 4. Rolloff spettrale e Valence\n",
    "ax4.plot(features_df['time'], features_df['rolloff_smooth'], color='brown', label='Rolloff spettrale')\n",
    "ax4.set_title('Rolloff spettrale e Valence nel tempo')\n",
    "ax4.set_xlabel('Tempo (secondi)')\n",
    "ax4.set_ylabel('Rolloff normalizzato', color='brown')\n",
    "ax4.tick_params(axis='y', labelcolor='brown')\n",
    "ax4.grid(True)\n",
    "\n",
    "# Aggiungiamo Valence sullo stesso grafico (asse y secondario)\n",
    "ax4_twin = ax4.twinx()\n",
    "ax4_twin.plot(features_df['time'], features_df['valence_smooth'], color='green', label='Valence')\n",
    "ax4_twin.set_ylabel('Valence', color='green')\n",
    "ax4_twin.tick_params(axis='y', labelcolor='green')\n",
    "ax4_twin.set_ylim(0, 1)\n",
    "\n",
    "# Aggiungiamo una legenda\n",
    "lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analisi della correlazione tra caratteristiche audio ed emozioni\n",
    "\n",
    "Calcoliamo e visualizziamo la correlazione tra le caratteristiche audio estratte e le annotazioni emozionali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcoliamo la matrice di correlazione\n",
    "correlation_columns = ['rms_smooth', 'centroid_smooth', 'contrast_smooth', 'rolloff_smooth', 'arousal_smooth', 'valence_smooth']\n",
    "correlation_matrix = features_df[correlation_columns].corr()\n",
    "\n",
    "# Visualizziamo la matrice di correlazione come heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.2f')\n",
    "plt.title('Correlazione tra caratteristiche audio ed emozioni')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estraiamo e visualizziamo le correlazioni specifiche con arousal e valence\n",
    "arousal_correlations = correlation_matrix['arousal_smooth'].drop(['arousal_smooth', 'valence_smooth'])\n",
    "valence_correlations = correlation_matrix['valence_smooth'].drop(['arousal_smooth', 'valence_smooth'])\n",
    "\n",
    "# Creiamo un DataFrame per facilitare la visualizzazione\n",
    "emotion_correlations = pd.DataFrame({\n",
    "    'Arousal': arousal_correlations,\n",
    "    'Valence': valence_correlations\n",
    "})\n",
    "\n",
    "# Visualizziamo le correlazioni come grafico a barre\n",
    "plt.figure(figsize=(12, 6))\n",
    "emotion_correlations.plot(kind='bar', color=['red', 'green'])\n",
    "plt.title('Correlazione tra caratteristiche audio ed emozioni')\n",
    "plt.xlabel('Caratteristica audio')\n",
    "plt.ylabel('Coefficiente di correlazione')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.legend(title='Emozione')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusioni\n",
    "\n",
    "In questo notebook, abbiamo esplorato come le caratteristiche audio estratte con Librosa si correlano con le emozioni (arousal e valence) nel tempo. Abbiamo visto come:\n",
    "\n",
    "1. L'energia (RMS) tende a correlarsi con l'arousal, indicando che brani pi√π energici sono percepiti come pi√π eccitanti\n",
    "2. Il centroide spettrale (brillantezza) pu√≤ correlarsi con la valence, suggerendo che suoni pi√π brillanti possono essere percepiti come pi√π positivi\n",
    "3. Il contrasto spettrale e il rolloff spettrale mostrano correlazioni variabili con le emozioni\n",
    "\n",
    "Queste analisi dimostrano come l'analisi temporale delle caratteristiche audio possa fornire informazioni preziose sulla percezione emotiva della musica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
