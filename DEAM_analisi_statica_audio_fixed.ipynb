{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Analisi delle Caratteristiche Audio a Livello di Canzone con Librosa üéµ\n",
    "\n",
    "Questo notebook si concentra sull'estrazione e analisi delle caratteristiche audio aggregate a livello di canzone intera, mostrando come le propriet√† sonore complessive si correlano con le annotazioni emotive statiche (arousal e valence) del dataset DEAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparazione dell'ambiente\n",
    "\n",
    "Importiamo le librerie necessarie per l'analisi audio e la visualizzazione dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa  # Libreria principale per l'analisi audio\n",
    "import librosa.display  # Per visualizzare i grafici audio\n",
    "import matplotlib.pyplot as plt  # Per creare grafici\n",
    "import numpy as np  # Per calcoli matematici\n",
    "import pandas as pd  # Per gestire i dati in formato tabellare\n",
    "import seaborn as sns  # Per grafici statistici pi√π avanzati\n",
    "from scipy import stats  # Per calcoli statistici\n",
    "\n",
    "# Impostiamo alcune opzioni di visualizzazione\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Dimensione predefinita dei grafici\n",
    "plt.rcParams['font.size'] = 12  # Dimensione del testo nei grafici\n",
    "sns.set_theme(style='whitegrid')  # Stile dei grafici seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento delle annotazioni statiche\n",
    "\n",
    "Carichiamo le annotazioni statiche medie (static_annotations_averaged_songs) dal dataset DEAM. Queste annotazioni contengono i valori medi di arousal e valence per ogni canzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso del file delle annotazioni statiche (da modificare in base alla posizione effettiva)\n",
    "# Nel dataset DEAM, questo file si trova tipicamente nella cartella 'annotations/static_annotations/static_annotations_averaged_songs/'\n",
    "# Per questo esempio, assumiamo che il file sia nella cartella 'DEAM_annotations'\n",
    "annotations_path = 'DEAM_annotations/static_annotations_averaged_songs.csv'\n",
    "\n",
    "# Verifichiamo se il file esiste\n",
    "if not os.path.exists(annotations_path):\n",
    "    print(f\"Il file {annotations_path} non esiste. Utilizzeremo dati di esempio.\")\n",
    "    # Creiamo dati di esempio per le annotazioni statiche\n",
    "    # In un caso reale, questi dati verrebbero caricati dal dataset DEAM\n",
    "    song_ids = [f'song_{i}' for i in range(1, 21)]  # 20 canzoni di esempio\n",
    "    \n",
    "    # Generiamo valori casuali di arousal e valence per ogni canzone\n",
    "    np.random.seed(42)  # Per riproducibilit√†\n",
    "    arousal_mean = np.random.uniform(0.3, 0.8, len(song_ids))\n",
    "    valence_mean = np.random.uniform(0.2, 0.9, len(song_ids))\n",
    "    \n",
    "    # Creiamo un DataFrame per le annotazioni statiche\n",
    "    static_annotations = pd.DataFrame({\n",
    "        'song_id': song_ids,\n",
    "        'arousal_mean': arousal_mean,\n",
    "        'valence_mean': valence_mean\n",
    "    })\n",
    "else:\n",
    "    # Carichiamo le annotazioni statiche dal file CSV\n",
    "    static_annotations = pd.read_csv(annotations_path)\n",
    "    print(f\"Caricate {len(static_annotations)} annotazioni statiche.\")\n",
    "\n",
    "# Mostriamo le prime righe del DataFrame\n",
    "static_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definizione della funzione per estrarre caratteristiche audio a livello di canzone\n",
    "\n",
    "Definiamo una funzione che estrae caratteristiche audio aggregate per l'intera canzone, invece che per segmenti temporali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_features(audio_path, duration=None):\n",
    "    \"\"\"\n",
    "    Estrae caratteristiche audio aggregate per l'intera canzone.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_path : str\n",
    "        Percorso del file audio\n",
    "    duration : float, optional\n",
    "        Durata in secondi da caricare (None per caricare l'intero file)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : dict\n",
    "        Dizionario con le caratteristiche audio estratte\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carichiamo il file audio\n",
    "        y, sr = librosa.load(audio_path, duration=duration)\n",
    "        \n",
    "        # Calcoliamo le caratteristiche audio per l'intera canzone\n",
    "        # 1. Energia (RMS)\n",
    "        rms = np.mean(librosa.feature.rms(y=y)[0])\n",
    "        \n",
    "        # 2. Centroide spettrale (brillantezza)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0])\n",
    "        \n",
    "        # 3. Contrasto spettrale (differenza tra picchi e valli)\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr)[0])\n",
    "        \n",
    "        # 4. Rolloff spettrale (distribuzione dell'energia)\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)[0])\n",
    "        \n",
    "        # 5. Zero crossing rate (quante volte il segnale attraversa lo zero)\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y)[0])\n",
    "        \n",
    "        # 6. Tempo (BPM)\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        \n",
    "        # 7. MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_means = np.mean(mfccs, axis=1)\n",
    "        \n",
    "        # 8. Chroma features (rappresentazione delle classi di altezza)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma)\n",
    "        \n",
    "        # Creiamo un dizionario con tutte le caratteristiche estratte\n",
    "        features = {\n",
    "            'rms': rms,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'spectral_contrast': spectral_contrast,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'zero_crossing_rate': zero_crossing_rate,\n",
    "            'tempo': tempo,\n",
    "            'chroma_mean': chroma_mean\n",
    "        }\n",
    "        \n",
    "        # Aggiungiamo i coefficienti MFCC\n",
    "        for i, mfcc_val in enumerate(mfcc_means):\n",
    "            features[f'mfcc_{i+1}'] = mfcc_val\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'estrazione delle caratteristiche per {audio_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estrazione delle caratteristiche audio per un insieme di canzoni\n",
    "\n",
    "Ora estraiamo le caratteristiche audio per un insieme di canzoni e le combiniamo con le annotazioni statiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso della cartella contenente i file audio (da modificare in base alla posizione effettiva)\n",
    "audio_folder = 'DEAM_audio/MEMD_audio/'\n",
    "\n",
    "# Verifichiamo se la cartella esiste\n",
    "if not os.path.exists(audio_folder):\n",
    "    print(f\"La cartella {audio_folder} non esiste. Utilizzeremo dati di esempio.\")\n",
    "    # Creiamo dati di esempio per le caratteristiche audio\n",
    "    # In un caso reale, queste caratteristiche verrebbero estratte dai file audio\n",
    "    \n",
    "    # Utilizziamo gli stessi song_ids delle annotazioni statiche\n",
    "    song_ids = static_annotations['song_id'].tolist()\n",
    "    \n",
    "    # Generiamo valori casuali per le caratteristiche audio\n",
    "    np.random.seed(42)  # Per riproducibilit√†\n",
    "    features_data = []\n",
    "    \n",
    "    for song_id in song_ids:\n",
    "        features = {\n",
    "            'song_id': song_id,\n",
    "            'rms': np.random.uniform(0.05, 0.25),\n",
    "            'spectral_centroid': np.random.uniform(1000, 3000),\n",
    "            'spectral_contrast': np.random.uniform(1, 7),\n",
    "            'spectral_rolloff': np.random.uniform(2000, 8000),\n",
    "            'zero_crossing_rate': np.random.uniform(0.05, 0.2),\n",
    "            'tempo': np.random.uniform(80, 160),\n",
    "            'chroma_mean': np.random.uniform(0.2, 0.7)\n",
    "        }\n",
    "        \n",
    "        # Aggiungiamo i coefficienti MFCC\n",
    "        for i in range(13):\n",
    "            features[f'mfcc_{i+1}'] = np.random.uniform(-40, 40)\n",
    "            \n",
    "        features_data.append(features)\n",
    "    \n",
    "    # Creiamo un DataFrame con le caratteristiche audio\n",
    "    audio_features_df = pd.DataFrame(features_data)\n",
    "    \n",
    "else:\n",
    "    # Troviamo tutti i file audio nella cartella\n",
    "    audio_files = glob.glob(os.path.join(audio_folder, '*.mp3'))\n",
    "    print(f\"Trovati {len(audio_files)} file audio.\")\n",
    "    \n",
    "    # Estraiamo le caratteristiche audio per ogni file\n",
    "    features_data = []\n",
    "    \n",
    "    for audio_file in audio_files[:20]:  # Limitiamo a 20 file per semplicit√†\n",
    "        # Estraiamo l'ID della canzone dal nome del file\n",
    "        song_id = os.path.basename(audio_file).split('.')[0]\n",
    "        \n",
    "        # Estraiamo le caratteristiche audio\n",
    "        features = extract_song_features(audio_file, duration=60)  # Limitiamo a 60 secondi per semplicit√†\n",
    "        \n",
    "        if features is not None:\n",
    "            # Aggiungiamo l'ID della canzone\n",
    "            features['song_id'] = song_id\n",
    "            features_data.append(features)\n",
    "    \n",
    "    # Creiamo un DataFrame con le caratteristiche audio\n",
    "    audio_features_df = pd.DataFrame(features_data)\n",
    "\n",
    "# Combiniamo le caratteristiche audio con le annotazioni statiche\n",
    "# Utilizziamo una fusione (merge) sui song_id\n",
    "combined_df = pd.merge(audio_features_df, static_annotations, on='song_id', how='inner')\n",
    "\n",
    "print(f\"Dataset combinato: {len(combined_df)} canzoni con caratteristiche audio e annotazioni emotive.\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analisi esplorativa delle caratteristiche audio e delle annotazioni emotive\n",
    "\n",
    "Esploriamo la distribuzione delle caratteristiche audio e delle annotazioni emotive nel nostro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiche descrittive delle caratteristiche audio e delle annotazioni emotive\n",
    "# Escludiamo la colonna song_id che √® categorica\n",
    "numeric_columns = combined_df.columns.drop('song_id')\n",
    "combined_df[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la distribuzione delle annotazioni emotive (arousal e valence)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Creiamo un grafico a dispersione (scatter plot) di arousal vs valence\n",
    "plt.scatter(combined_df['valence_mean'], combined_df['arousal_mean'], alpha=0.7, s=100)\n",
    "\n",
    "# Aggiungiamo etichette per ogni punto (opzionale)\n",
    "for i, song_id in enumerate(combined_df['song_id']):\n",
    "    plt.annotate(song_id, (combined_df['valence_mean'].iloc[i], combined_df['arousal_mean'].iloc[i]),\n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Aggiungiamo linee che dividono lo spazio in quadranti\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Aggiungiamo etichette per i quadranti\n",
    "plt.text(0.25, 0.75, 'Tensione\n(Alta arousal, Bassa valence)', ha='center', va='center', fontsize=10)\n",
    "plt.text(0.75, 0.75, 'Eccitazione\n(Alta arousal, Alta valence)', ha='center', va='center', fontsize=10)\n",
    "plt.text(0.25, 0.25, 'Depressione\n(Bassa arousal, Bassa valence)', ha='center', va='center', fontsize=10)\n",
    "plt.text(0.75, 0.25, 'Rilassamento\n(Bassa arousal, Alta valence)', ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.title('Distribuzione delle canzoni nello spazio emotivo (Arousal vs Valence)')\n",
    "plt.xlabel('Valence (positivit√†)')\n",
    "plt.ylabel('Arousal (eccitazione)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la distribuzione di alcune caratteristiche audio chiave\n",
    "key_features = ['rms', 'spectral_centroid', 'spectral_contrast', 'tempo', 'zero_crossing_rate']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(key_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(combined_df[feature], kde=True)\n",
    "    plt.title(f'Distribuzione di {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequenza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analisi della correlazione tra caratteristiche audio ed emozioni\n",
    "\n",
    "Calcoliamo e visualizziamo la correlazione tra le caratteristiche audio estratte e le annotazioni emotive statiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selezioniamo le colonne numeriche per l'analisi di correlazione\n",
    "# Escludiamo la colonna song_id che √® categorica\n",
    "correlation_columns = combined_df.columns.drop('song_id')\n",
    "\n",
    "# Calcoliamo la matrice di correlazione\n",
    "correlation_matrix = combined_df[correlation_columns].corr()\n",
    "\n",
    "# Visualizziamo la matrice di correlazione completa come heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Matrice di correlazione completa')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estraiamo e visualizziamo solo le correlazioni con arousal_mean e valence_mean\n",
    "arousal_correlations = correlation_matrix['arousal_mean'].drop(['arousal_mean', 'valence_mean'])\n",
    "valence_correlations = correlation_matrix['valence_mean'].drop(['arousal_mean', 'valence_mean'])\n",
    "\n",
    "# Ordiniamo le correlazioni per valore assoluto decrescente\n",
    "arousal_correlations = arousal_correlations.reindex(arousal_correlations.abs().sort_values(ascending=False).index)\n",
    "valence_correlations = valence_correlations.reindex(valence_correlations.abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Creiamo un DataFrame per facilitare la visualizzazione\n",
    "emotion_correlations = pd.DataFrame({\n",
    "    'Arousal': arousal_correlations,\n",
    "    'Valence': valence_correlations\n",
    "})\n",
    "\n",
    "# Visualizziamo le correlazioni come grafico a barre\n",
    "plt.figure(figsize=(12, 10))\n",
    "emotion_correlations.plot(kind='barh', color=['red', 'green'])\n",
    "plt.title('Correlazione tra caratteristiche audio ed emozioni')\n",
    "plt.xlabel('Coefficiente di correlazione')\n",
    "plt.ylabel('Caratteristica audio')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.legend(title='Emozione')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizzazione delle relazioni tra caratteristiche audio ed emozioni\n",
    "\n",
    "Visualizziamo le relazioni tra le caratteristiche audio pi√π correlate con arousal e valence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troviamo le caratteristiche pi√π correlate con arousal e valence\n",
    "top_arousal_features = arousal_correlations.abs().sort_values(ascending=False).head(4).index.tolist()\n",
    "top_valence_features = valence_correlations.abs().sort_values(ascending=False).head(4).index.tolist()\n",
    "\n",
    "# Creiamo scatter plot per le caratteristiche pi√π correlate con arousal\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(top_arousal_features):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.regplot(x=feature, y='arousal_mean', data=combined_df, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
    "    plt.title(f'{feature} vs Arousal (r={arousal_correlations[feature]:.2f})')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Arousal')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Caratteristiche audio pi√π correlate con Arousal', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Creiamo scatter plot per le caratteristiche pi√π correlate con valence\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(top_valence_features):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.regplot(x=feature, y='valence_mean', data=combined_df, scatter_kws={'alpha':0.6}, line_kws={'color':'green'})\n",
    "    plt.title(f'{feature} vs Valence (r={valence_correlations[feature]:.2f})')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Valence')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Caratteristiche audio pi√π correlate con Valence', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusioni\n",
    "\n",
    "In questo notebook, abbiamo esplorato come le caratteristiche audio estratte con Librosa si correlano con le emozioni (arousal e valence) a livello di canzone intera. Abbiamo visto come:\n",
    "\n",
    "1. Alcune caratteristiche audio come RMS (energia), centroide spettrale (brillantezza) e tempo possono essere buoni predittori delle emozioni percepite\n",
    "2. L'arousal (eccitazione) tende a correlarsi maggiormente con caratteristiche legate all'energia e al ritmo\n",
    "3. La valence (positivit√†) tende a correlarsi maggiormente con caratteristiche legate alla tonalit√† e alla brillantezza del suono\n",
    "4. L'analisi a livello di canzone intera permette di identificare pattern pi√π generali rispetto all'analisi temporale\n",
    "\n",
    "Questo approccio di analisi a livello di canzone intera √® complementare all'analisi temporale e permette di studiare un dataset pi√π ampio di brani, fornendo una visione pi√π completa di come le caratteristiche audio complessive influenzano la percezione emotiva della musica."
   ]
  }
 ]
}